{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop - Human in the Loop for SageMaker Models - Module 3\n",
    "\n",
    "Now that we have used the model to generate prediction on some random out-of-sample images and got unsatisfactory prediction (low probability). We also demonstrated how to use Amazon Augmented AI to review and label the image based on custom criteria. Next step in a typical machine learning life cycle is to include these cases with which the model has trouble in the next batch of training data for retraining purposes so that the model can now learn from a set of new training data to improve the model. In machine learning we call it [incremental training](https://docs.aws.amazon.com/sagemaker/latest/dg/incremental-training.html).\n",
    "\n",
    "There are [three ways](https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection.html#object-detection-inputoutput) to supply the image data and annotation to SageMaker built-in object detection algorithm. We trained our original model with the RecordIO format as we converted the PASCAL VOC images and annotations into RecordIO format. If you want to create a custom RecordIO data, you could follow the steps outlined [here](https://gluon-cv.mxnet.io/build/examples_datasets/detection_custom.html). Alternatively, SageMaker built-in object detection algorithm also takes JSON file as annotation along with your JPEG/PNG images. You could create one JSON file per image as in **Train with the Image Format** in the [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection.html#object-detection-inputoutput), or take an advantage of [pipe mode](https://aws.amazon.com/blogs/machine-learning/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker/) enabled by using [Augmented Manifest](https://docs.aws.amazon.com/sagemaker/latest/dg/augmented-manifest.html) as input format. Pipe mode accelerate overall model training time up to 35% by streaming the data into the training algorithm while it is running instead of copying data to the EBS volume attached to the training instance. We could construct augmented manifest file from the A2I output with the following function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "BUCKET = sess.default_bucket()\n",
    "OUTPUT_PATH = f's3://{BUCKET}/a2i-results'\n",
    "MODEL_PATH = f's3://{BUCKET}/model'\n",
    "object_categories = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', \n",
    "                     'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', \n",
    "                     'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client = boto3.client('sagemaker')\n",
    "a2i = boto3.client('sagemaker-a2i-runtime')\n",
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_categories_dict = {str(i): j for i, j in enumerate(object_categories)}\n",
    "\n",
    "def convert_a2i_to_augmented_manifest(a2i_output):\n",
    "    annotations = []\n",
    "    confidence = []\n",
    "    for i, bbox in enumerate(a2i_output['humanAnswers'][0]['answerContent']['annotatedResult']['boundingBoxes']):\n",
    "        object_class_key = [key for (key, value) in object_categories_dict.items() if value == bbox['label']][0]\n",
    "        obj = {'class_id': int(object_class_key), \n",
    "               'width': bbox['width'],\n",
    "               'top': bbox['top'],\n",
    "               'height': bbox['height'],\n",
    "               'left': bbox['left']}\n",
    "        annotations.append(obj)\n",
    "        confidence.append({'confidence': 1})\n",
    "\n",
    "    # We set \"a2i-retraining\" as the attribute name for this dataset. This will later be used in setting the training data\n",
    "    augmented_manifest={'source-ref': a2i_output['inputContent']['taskObject'],\n",
    "                        'a2i-retraining': {'annotations': annotations,\n",
    "                                           'image_size': [{'width': a2i_output['humanAnswers'][0]['answerContent']['annotatedResult']['inputImageProperties']['width'],\n",
    "                                                           'depth':3,\n",
    "                                                           'height': a2i_output['humanAnswers'][0]['answerContent']['annotatedResult']['inputImageProperties']['height']}]},\n",
    "                        'a2i-retraining-metadata': {'job-name': 'a2i/%s' % a2i_output['humanLoopName'],\n",
    "                                                    'class-map': object_categories_dict,\n",
    "                                                    'human-annotated':'yes',\n",
    "                                                    'objects': confidence,\n",
    "                                                    'creation-date': a2i_output['humanAnswers'][0]['submissionTime'],\n",
    "                                                    'type':'groundtruth/object-detection'}}\n",
    "    return augmented_manifest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will take an A2I output json and result in a json object that is compatible to how Amazon SageMaker Ground Truth outputs the result and how SageMaker built-in object detection algorithm expects from the input. In order to create a cohort of training images from all the images re-labeled by human reviewers in A2I console. You can loop through all the A2I output, convert the json file, and concatenate them into a JSON Lines file, with each line represents results of one image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowDefinitionName = 'fd-sagemaker-object-detection-demo'\n",
    "flowDefinitionArn = sagemaker_client.describe_flow_definition(\n",
    "    FlowDefinitionName=flowDefinitionName\n",
    ")['FlowDefinitionArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2i.list_human_loops(FlowDefinitionArn=flowDefinitionArn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_loops = a2i.list_human_loops(FlowDefinitionArn=flowDefinitionArn)\n",
    "completed_human_loops = []\n",
    "\n",
    "for loop in human_loops['HumanLoopSummaries']:\n",
    "    resp = a2i.describe_human_loop(HumanLoopName=loop['HumanLoopName'])\n",
    "    print(f'HumanLoop Status: {resp[\"HumanLoopStatus\"]}')\n",
    "    print(f'HumanLoop Output Destination: {resp[\"HumanLoopOutput\"]}')\n",
    "    print('\\n')\n",
    "\n",
    "    if resp[\"HumanLoopStatus\"] == \"Completed\":\n",
    "        completed_human_loops.append(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=[]\n",
    "with open('augmented.manifest', 'w') as outfile:\n",
    "    # convert the a2i json to augmented manifest for each human loop output\n",
    "    for resp in completed_human_loops:\n",
    "        splitted_string = re.split('s3://' +  BUCKET + '/', resp['HumanLoopOutput']['OutputS3Uri'])\n",
    "        output_bucket_key = splitted_string[1]\n",
    "\n",
    "        response = s3.get_object(Bucket=BUCKET, Key=output_bucket_key)\n",
    "        content = response[\"Body\"].read()\n",
    "        json_output = json.loads(content)\n",
    "        \n",
    "        # convert using the function\n",
    "        augmented_manifest = convert_a2i_to_augmented_manifest(json_output)\n",
    "        print(json.dumps(augmented_manifest))\n",
    "        json.dump(augmented_manifest, outfile)\n",
    "        outfile.write('\\n')\n",
    "        output.append(augmented_manifest)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at how Json Lines looks like\n",
    "!head -n2 augmented.manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload the manifest file to S3\n",
    "!aws s3 cp augmented.manifest {OUTPUT_PATH}/augmented.manifest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to training with Ground Truth output augmented manifest file outlined in this [blog](https://aws.amazon.com/blogs/machine-learning/easily-train-models-using-datasets-labeled-by-amazon-sagemaker-ground-truth/), once we have collected enough data points, we can construct a new `Estimator` for incremental training. \n",
    "\n",
    "For incremental training, the choice of hyperparameters becomes critical. Since we are continue the learning and optimization from the last model, an appropriate starting `learning_rate`, for example, would again need to be determined. But as a rule of thumb, even with the introduction of new, unseen data, we should start out the incremental training with a smaller `learning_rate` and different learning rate schedule (`lr_scheduler_factor` and `lr_scheduler_step`) than that of the previous training job as the optimization has previously reached to a more stable state with reduced learning rate. We should see a similar mAP performance on the original validation dataset in the first epoch in the incremental training. \n",
    "\n",
    "We here will be using the hyperparameters exactly the same as how the first model was trained, with the following exceptions\n",
    "\n",
    "- smaller learning rate (`learning_rate` was 0.001, now 0.0001)\n",
    "- using the weights from the trained model instead of pre-trained weights that comes with the algorithm (`use_pretrained_model=0`).\n",
    "\n",
    "Note that the following working code snippet is meant to demonstrate how to set up the A2I output for training in SageMaker with object detection algorithm. Incremental training with merely 1 or 2 new samples and untuned hyperparameters, would not yield a meaning model, if not experiencing [catastrophic forgetting](https://en.wikipedia.org/wiki/Catastrophic_interference)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# path definition\n",
    "s3_train_data = f'{OUTPUT_PATH}/augmented.manifest'\n",
    "# Reusing the training data for validation here for demonstration purposes\n",
    "# but in practice you should provide a set of data that you want to validate the training against\n",
    "s3_validation_data = s3_train_data \n",
    "s3_output_location = f'{OUTPUT_PATH}/incremental-training'\n",
    "\n",
    "num_training_samples = len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_model_data_s3_uri = 's3://aws-sagemaker-augmented-ai-example/model/model.tar.gz'\n",
    "\n",
    "!aws s3 cp {source_model_data_s3_uri} {MODEL_PATH}/model.tar.gz\n",
    "\n",
    "model_data_s3_uri = f'{MODEL_PATH}/model.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the input data\n",
    "train_data = sagemaker.inputs.TrainingInput(s3_train_data, \n",
    "                                            distribution='FullyReplicated', \n",
    "                                            content_type='application/x-recordio',\n",
    "                                            record_wrapping='RecordIO',\n",
    "                                            s3_data_type='AugmentedManifestFile', \n",
    "                                            attribute_names=['source-ref', 'a2i-retraining'])\n",
    "\n",
    "validation_data = sagemaker.inputs.TrainingInput(s3_validation_data, \n",
    "                                                 distribution='FullyReplicated', \n",
    "                                                 content_type='application/x-recordio',\n",
    "                                                 record_wrapping='RecordIO',\n",
    "                                                 s3_data_type='AugmentedManifestFile', \n",
    "                                                 attribute_names=['source-ref', 'a2i-retraining'])\n",
    "\n",
    "# Use the output model from the original training job.  \n",
    "model_data = sagemaker.inputs.TrainingInput(model_data_s3_uri, \n",
    "                                            distribution='FullyReplicated',\n",
    "                                            content_type='application/x-sagemaker-model', \n",
    "                                            s3_data_type='S3Prefix',\n",
    "                                            input_mode = 'File')\n",
    "\n",
    "data_channels = {'train': train_data, \n",
    "                 'validation': validation_data,\n",
    "                 'model': model_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = sagemaker.image_uris.retrieve('object-detection', 'us-east-1', version='1')\n",
    "\n",
    "# Create a model object set to using \"Pipe\" mode because we are inputing augmented manifest files.\n",
    "new_od_model = sagemaker.estimator.Estimator(image, # same object detection image that we used for model hosting  \n",
    "                                             role, \n",
    "                                             instance_count=1, \n",
    "                                             instance_type='ml.p3.2xlarge', \n",
    "                                             volume_size = 50, \n",
    "                                             max_run = 360000, \n",
    "                                             input_mode = 'Pipe',\n",
    "                                             output_path=s3_output_location, \n",
    "                                             sagemaker_session=sess) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same set of hyperparameters from the original training job\n",
    "new_od_model.set_hyperparameters(base_network='resnet-50',\n",
    "                                 use_pretrained_model=0, # we are going to use our own model\n",
    "                                 num_classes=20,\n",
    "                                 learning_rate=0.0001,   # smaller learning rate for a more stable search\n",
    "                                 mini_batch_size=1,\n",
    "                                 epochs=1,               # 1 for demo purposes\n",
    "                                 lr_scheduler_step='3,6',\n",
    "                                 lr_scheduler_factor=0.1,\n",
    "                                 optimizer='sgd',\n",
    "                                 momentum=0.9,\n",
    "                                 weight_decay=0.0005,\n",
    "                                 overlap_threshold=0.5,\n",
    "                                 nms_threshold=0.45,\n",
    "                                 image_shape=300,\n",
    "                                 label_width=350,\n",
    "                                 num_training_samples=num_training_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_od_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, you would get a new model in the `s3_output_location`, you can deploy it to a new endpoint or modify an endpoint without taking models that are already deployed into production out of service. For example, you can add new model variants, update the ML Compute instance configurations of existing model variants, or change the distribution of traffic among model variants. To modify an endpoint, you provide a new endpoint configuration. Amazon SageMaker implements the changes without any downtime. For more information, see [UpdateEndpoint](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_UpdateEndpoint.html) and [UpdateEndpointWeightsAndCapacities](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_UpdateEndpointWeightsAndCapacities.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More on incremental training\n",
    "It is recommended to perform a search over the hyperparameter space for your incremental training with [hyperparameter tuning](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html) for an optimal set of hyperparameters, especially the ones related to learning rate: `learning_rate`, `lr_scheduler_factor` and `lr_scheduler_step` from the SageMaker object detection algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up\n",
    "\n",
    "Remember to exeute the last cells in module 1 and module 2"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
